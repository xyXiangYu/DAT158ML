{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><span style=\"font-size:150%\">Work in progress</span></b>\n",
    "\n",
    "version August 17th, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes through some core machine learning concepts connected to **classification**. It's based on the code in Chapter 3 of the textbook. See also here: https://github.com/ageron/handson-ml/blob/master/03_classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification the computer is asked to decide which of N classes an input belongs to. Think of the fishes from the first lecture. Or classifying images according to the objects that appear in the image (i.e. cat vs dogs vs cars, hot dog vs nothotdog).\n",
    "\n",
    "It's about finding a good function from the inputs $X$ to the a set of labels:\n",
    "\n",
    "$$f: X \\rightarrow \\{1, ..., N\\}, \\qquad \\mbox{classification}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is one of the two main forms of supervised learning. The other one is **regression**, which is about finding a function that maps from the inputs to a continuous set of numbers. Think of the housing prices from Assignment 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f: X \\rightarrow \\mathbb{R}, \\qquad \\mbox{regression}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you decide whether you have a classification or a regression problem? Ask yourself\n",
    "> Is there continuity in the possible predicitons?\n",
    "\n",
    "Predicting a price of 1.500.000 versus 1.500.001 makes a small difference and you should probably treat this as a regression problem. Predicting that something contains object #12 versus object #13 likely makes a huge difference, which suggests that you're dealing with a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To suppress some warnings below. Nothing important, just to make some outputs cleaner.\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we add the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To automatically reload modules defined in external files.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# To display plots directly in the notebook:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and import our standard framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the notebook reproducible\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of some possible issues with loading the data below we import this convenience function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import fetch_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll base our discussion on a famous benchmark dataset: **MNIST**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<centering>\n",
    "<img src=\"assets/MnistExamples.png\">\n",
    "</centering>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It consists of 70.000 examples of handwritten digits. It's been called \"the machine learning equivalent of fruit flies\": it's simple, but not *too* simple, and *very* well-studied. Have a look at https://en.wikipedia.org/wiki/MNIST_database and http://yann.lecun.com/exdb/mnist/ for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to construct a system that can take an image from MNIST as input and produce the correct digit 0, ..., 9 as output. This is a **multi-class** system:\n",
    "<centering>\n",
    "<img src=\"assets/goal.png\">\n",
    "</centering>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a difficult problem.. Can you come up with features that characterizes all the number 2's, but none of the other digits? How can you program rules that detect only 2's?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<centering>\n",
    "<img width=40% src=\"assets/mnist-difficult.png\">\n",
    "</centering>\n",
    "<br>\n",
    "<span style=\"font-size: 70%\">Image from G. Hinton's Coursera course [Neural Networks for Machine Learning](https://www.coursera.org/learn/neural-networks)</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database <a href=\"http://mldata.org\">mldata.org</a> is a convenient source of machine learning data containing MNIST among others. Scikit-learn has a method that can be used to download data from mldata.org:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?fetch_mldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_mldata(\"MNIST original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a [Python dictionary](https://www.datacamp.com/community/tutorials/python-dictionary-tutorial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are stored under `mnist.data` and the labels in `mnist.target`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist.data\n",
    "y = mnist.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the 70.000 images are of size 28*28 = 784:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...stored as Numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we should have a look at the data. That's particularly useful when dealing with images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit = X[36000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are vectors of size 784:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display the image we convert to a 28x28 array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_image = some_digit.reshape(28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a small section of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_image[15:20,15:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers represent greyscale values. 0 is white, 255 is black. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(some_digit_image, cmap=matplotlib.cm.binary) # These are greyscale images. Specified with cmap.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a smalle function that plots MNIST images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary)\n",
    "    plt.axis(\"Off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digit(X[1234])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and plot a random selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=10\n",
    "to_plot = random.choices(X, k=num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(14,14))\n",
    "for i in range(num):\n",
    "    plt.subplot(1,num,i+1)\n",
    "    plot_digit(to_plot[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the first 60.000 images as our training data, and the last 10.000 as test (this is the standard division for MNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later we'll want to select subsets of the training data. To increase our chances that each subset contains all the digits we shuffle the data. We can use Numpy's `random.permutation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.permutation(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before:\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=seed)\n",
    "shuffled_indices = np.random.permutation(len(X_train))\n",
    "X_train, y_train = X_train[shuffled_indices], y_train[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After:\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A binary classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by constructing a **binary classifier**: a system that aims to distinguish 5's from non-5's. This makes things easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5) # `True` if the label is 5, else `False`\n",
    "y_test_5 = (y_test == 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of choices when building our model, something we'll learn more about in Part 3 of the course. For now, let's use `SGDClassifier` as a \"black box\", without studying how it works behind the scenes (but feel free to read more about the model [here](http://scikit-learn.org/stable/modules/sgd.html)). Our focus is on classification in general, not specific models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model:\n",
    "sgd_clf = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model:\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now trained on the training data, and we can use it to predict whether a digit is a 5 or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(X):\n",
    "    plot_digit(X)\n",
    "    plt.title('Input image')\n",
    "    plt.show()\n",
    "    \n",
    "    # Predict\n",
    "    pred = sgd_clf.predict([X]) \n",
    "    print(f'Prediction: {pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(X_test[5300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(X_test[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(X_test[5212])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By experimenting with various input images you'll notice that the model is correct some times, incorrect other. **How good is the model, really?** \n",
    "\n",
    "We need ways to evaluate and validate the models we construct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating models. \"Performance measures\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, when evaluating the performance of a model one should really ask \"what is the end goal for my system\"? We're supposed to create systems that are useful in some context, as part of a larger system, which typically has a higher-level goal that our system should aim to optimize. Perhaps it's worth sacrificing accuracy for speed, or not getting a lot of useless clicks that don't lead to sales. \n",
    "\n",
    "However, we won't think about these things in the following. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy and different kinds of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification a common metric is accuracy: what fraction of our predictions were correct? \n",
    "\n",
    "But there are different kinds of errors: if we classify something as belonging to the positive class we can either be correct (**true positive**) or incorrect (**false positive**). If we classify something as negative we can either be correct (**true negative**) or incorrect (**false negative**). \n",
    "\n",
    "Which kinds of errors we care most about depends on the task: if we're diagnosing a treatable condition in patients we should do everything we can to reduce the false negative rate. While perhaps still keeping an on the false positive rate because a positive diagnosis could lead to invasive and extensive further testing for the patient.\n",
    "\n",
    "In spam filtering we care most about not marking important emails as spam, i.e. we want a low false positive rate (non-spam marked as spam) even if it means a higher false negative rate (some spam emails ending up in our inbox). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see how we can measure all these kinds of errors below. But first, we must simulate incoming new, unseen data sets on which to make our measurements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting aside a subset of the training data (a hold-out set) as a test set we can use it to evaluate model perfomance. Remember (again!) we're not interested in how well our models perform on the training set, what we're really after is how well they generalize to unseen data. The test set simulates unseen data (and should therefore not be touched when constructing and tuning our models). \n",
    "\n",
    "We've seen this idea used earlier in the course, and we know how to do this using the `train_test_split` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn!**\n",
    "- Split `(X_train, y_train)` into two parts: one for training, one for evaluation of the model\n",
    "- Train your model again on the new training set, and compute its accuracy on the test set using the `score` method. \n",
    "- Is the result good? What accuracy would a random guesser have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A frequently used way to evaluate models is to do *cross-validation*. A very important advantage of this method over the one above is that it doesn't waste as much training data. Unless you have plenty of data, cross-validation is the preferred method for estimating model performance.\n",
    "\n",
    "Cross-validation provides a more thorough test than splitting the data into a training set and a test set. The `train_test_split` procedure sets aside a random subset of the data as a test set. If we're \"lucky\", all the difficult examples end up in the training set, while the test set contains only easy ones. That would lead to an overestimate of the true performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to randomly split the training set into several parts, so called *folds*. Say into K folds, for example. Then train a model K times, each time using a different fold for evaluation and training on the remaining K-1. The average score for the K runs is used to estimate the model's performance. \n",
    "\n",
    "This means that *each sample in the training set is part of the training set K-1 times and the evaluation set once*. \n",
    "\n",
    "<img src=\"assets/K-fold_cross_validation_EN.jpg\"><br>\n",
    "<span style=\"font-size:70%\">Image from <a href=\"https://commons.wikimedia.org/wiki/File:K-fold_cross_validation_EN.jpg\">Wikipedia</a></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the special case where K is set to the number of data points in the training set is called *leave-one-out*. Each fold is then a single sample.\n",
    "\n",
    "The result of this K-fold cross validation procedure is an array of K evaluation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(sgd_clf, X_train, y_train_5, cv=5, scoring=\"accuracy\")\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean accuracy for our model is roughly 94% &ndash; 96%, which looks pretty good! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but is it really? Remember, there are 10 digits in MNIST. A subset of the data is therefore expected to contain roughly 10% 5's and 90% non-5's. We're dealing with a highly **unbalanced dataset** (also called a *skewed dataset*).\n",
    "\n",
    "A model that's always predicting non-5's will have an accuracy of approximately 90%. That makes our model less impressive..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, there's a better way to evaluate the model than computing the accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix us a table summarizing the results from a classification model's predictions. It displays in what way your classifier is \"confused\": what kind of errors it tends to make. \n",
    "\n",
    "The idea is to count the number of times instances of a certain class is classified as the different classes in your problem. In our case, how often a 5 is misclassified as a non-5. \n",
    "\n",
    "It's particularly useful when there are more than two classes, as we shall see later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a set of predictions to compare with the true classes. We can get predicitons using `cross_val_predict`, which works in a similar to `cross_val_score` except that it returns predictions based on all the K folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first compute the confusion matrix and then explain what it tells us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train_5, y_train_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a convenient function for plotting the confusion matrix. Adapted from http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, ax=None,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues, labels=True):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    From \n",
    "    \"\"\"\n",
    "    \n",
    "    if not ax: fig, ax = plt.subplots(figsize=(8,6))\n",
    "    ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.setp(ax, xticks=tick_marks, xticklabels=classes, \n",
    "             yticks=tick_marks, yticklabels=classes,\n",
    "             title=title, xlabel=\"Predicted label\", \n",
    "             ylabel=\"True label\")\n",
    "    #ax.set_xticks(tick_marks, classes, rotation=45)\n",
    "    #plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if labels:\n",
    "        fmt = 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            ax.text(j, i, format(int(cm[i, j]), fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_confusion_matrix(cm, classes=['non-5',5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the matrix represents a class: non-5s and 5s. Each column represents a predicted class. \n",
    "\n",
    "The first row tells us that more than 50.000 images were correctly predicted non-5s (**true negatives**). There were 1000+ images predicted to be 5s but where actually non-5s (**false positives**). \n",
    "\n",
    "The second row tells us that around 1500 5's were wrongly labeled (**false negatives**), while the rest of the 5's were correct (**true positives**). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision and recall of a classifier are two very useful concepts worth understanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **precision is the proportion of the positive predictions that were actually correct**. In other words, if we write TP for the true positives and FP for the false positives:\n",
    "\n",
    "$$\\mbox{precision} = \\dfrac{TP}{TP+FP}$$\n",
    "\n",
    "If there are no false positives (which is great!), the precision is 1.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **recall is the proportion of actual positives that were identified correctly**. In other words, if we write FN for the false negatives, \n",
    "\n",
    "$$\\mbox{recall} = \\dfrac{TP}{TP + FN}$$\n",
    "\n",
    "If there are no false negatives (which is great!), the recall is 1.0. Recall is very often called *sensitivity*, and sometimes the *true positive rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The connection to the confusion matrix is clear:\n",
    "\n",
    "<img src=\"assets/cm_illustration.png\"><br>\n",
    "\n",
    "<span style=\"font-size:70%\"> Image is Figure 3-2 in the textbook</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a nice pictorial explanation of precision and recall (from Wikipedia):<br>\n",
    "<img src=\"assets/Precisionrecall.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the precision and recall of our classifier by hand from the confusion matrix (**do that!**), or using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that our classifier isn't particularly good. When predicting that something is a 5 it's only correct approximately 70% of the time. And it only detects about 80% of the 5's. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $F_1$ score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate a classification model you have to consider *both* precision and recall. If you only look at precision you can be fooled by a classifier that makes way too few positive predicitons, but when it does it's typically correct (leading to a very high precision). You also need it to pick up as many of the positive cases as possible (that is, have high recall). The same is true for high recall: it's not enough to detect all the postive examples, the model also has to make correct predictions for them! Otherwise it could just say that \"everything is 5!\". \n",
    "\n",
    "One way to combine precision and recall into one number is to use the **$F_1$ score**, which is a form of average of the two (in fact, what's called the *harmonic mean*):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F_1 = \\dfrac{2}{\\dfrac{1}{\\mbox{precision}} + \\dfrac{1}{\\mbox{recall}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An $F_1$ score of 1.0 means perfect precision *and* perfect recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision/recall tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, very often it's impossible to get *both* high precision and high recall. They are typically competing quantities: when one is high, the other is low. Improving the precision often lowers the recall, and vice-versa. You have to trade one for the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classifier with perfect recall is easy to construct: just predict that everything is a 5! Then there are no false negatives and the recall is 1.0. Of course the precision will be rubish. \n",
    "\n",
    "To get high precision, just have the classifier predict *one* 5 that it's really sure of, and the rest as non-5s. Then FP will likely be 0 and the precision 1.0. The recall will be really bad since there will be many FNs.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in action for our `SGDClassifier`. Behind the scenes, when we ran `sgd_clf.fit` on our training data the model finds a *hyperplane*, or a *boundary* in the data space that separates the 5's from the non-5's. \n",
    "\n",
    "When feeding `SGDClassifier` and data point it decides whether it is a 5 or a non-5 based on the *distance* from the data point to the hyperplane. By default, if the data point has a negative distance to the plane it's predicted to be from the negative class (a non-5). If it has positive distance it's predicted to be from the positive class (a 5). In other words, the **decision threshold** is set to 0 by default. The distance to the data point can be intepreted as how certain the model is about its prediction. The larger the distance, the more certain. \n",
    "\n",
    "But where you set this threshold **makes a big difference!**. We can see that visually here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/precisionrecalltradeoff.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and do some experiments with code on our example digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(some_digit.reshape(28,28), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sgd_clf has a method that computes the distance of a data point to the hyperplane constructed from the training data:\n",
    "y_example_score = sgd_clf.decision_function([some_digit])\n",
    "print(y_example_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "y_example_pred = (y_example_score > threshold)\n",
    "print(y_example_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 30000\n",
    "y_example_pred = (y_example_score > threshold)\n",
    "print(y_example_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that increasing the threshold makes the model less likely to classifiy something as a 5, which increases the precision but lowers the recall. Lowering the threshold makes the recall higher, but lowers the precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compute the decision function scores for all the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=5, method='decision_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 60.000 scores, one for each training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute and plot the precisions and recalls of these for various thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)\n",
    "    plt.legend(loc=\"upper left\", fontsize=16)\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.xlim([-700000, 700000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and select the threshold value that gives the best precision/recall tradeoff for your task. (**Note:** The choice of threshold should of course be done based on a validation data set, not the test data set! No tweaking of the model based on the test set performance!) \n",
    "\n",
    "Are false negatives very bad (like in medical diagnostics)? Use a low threshold to get high recall! Are false positives especially costly? Go for a threshold that gives you high precision!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very easy to create a classifier with any precision you want, as long as you don't care about recall. You just have to select a suitable threshold. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_precise = (y_scores > 200000)\n",
    "precision_score(y_train_5, y_train_pred_precise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train_5, y_train_pred_precise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Geron writes: \n",
    "> If someone says \"let's reach 90% precision\", you should ask, \"at what recall?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **ROC curve (reciever operating characteristic curve)** is a graph that's often used to evaulate binary classifiers (classifiers where there are only two classes). It's a plot of the **true positive rate** (which is another name for recall) versus the **false positive rate**:\n",
    "\n",
    "$$\\begin{align} TPR = \\dfrac{TP}{TP + FN}\\\\ \\\\ FPR = \\dfrac{FP}{FP + TN} \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the TPR and FPR for various thresholds using the `roc_curve` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and plot the ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    f = plt.figure(figsize=(8,6))\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('FPR', fontsize=14)\n",
    "    plt.ylabel('TPR (recall)', fontsize=14)\n",
    "    \n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really good classifier will be very close to the top left corner (high TPR and low FPR for any threshold), while a completely random classifier will be close to the dotted line (as many true positives as false positives for any threshold). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the ROC curve to compute how good the classifier is: if it's really good the area under the curve will be high (i.e. close to the maximum which is 1.0), if it's really bad (i.e random) it will be close to 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a good score? Well, not really. There are very few 5's compared to non-5's which means that a classifier that tends to predict 5's too rarely (i.e. low recall) still gets a seemingly okay ROC curve. In cases where you have an imbalanced dataset plotting the precision versus the recall can give you more information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    ax.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.setp(ax, xlabel=\"Recall\", ylabel=\"Precision\")\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    return ax\n",
    "    \n",
    "_ = plot_precision_vs_recall(precisions, recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you really want are classifiers whose precisiom-recall curve goes to the far upper right-hand corner. High precision *and* high recall. We notice that in this case we can get a recall of about 0.8 while still having an OK precision. Higher than that the precision drops quickly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have ways to compare classifiers: we can for example use the ROC curve and the ROCAUC (ROC area under the curve). The higer the ROCAUC, the closer the ROC curve comes to the top-left corner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try out our powers by making another model for our data: a random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n",
    "                                    method=\"predict_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\n",
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, \"b:\", linewidth=2, label=\"SGD\")\n",
    "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n",
    "plt.legend(loc=\"lower right\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_train_5, y_scores_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which is higher than "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...leading us to prefer the random forest model (unless of course other considerations than what's captured by the ROCAUC score are more important to us)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the two model's precision-recall curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions_forest, recalls_forest, thresholds_forest = precision_recall_curve(y_train_5, y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_precision_vs_recall(precisions, recalls)\n",
    "ax.plot(precisions_forest, recalls_forest)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which also leads us to prefer the random forest (we can get higher precisions at higher recall than for the SGDClassifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the ROC curve, we can compute the area under the precision-recall curve, also called *average precision* and use this number to assess the model. Often much more convenient than looking at the curve, and it also enables automatic model selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "aps_sgd = average_precision_score(y_train_5, y_scores)\n",
    "aps_rf = average_precision_score(y_train_5, y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The average precision score of SGDClassifier is: {aps_sgd}\\n')\n",
    "print(f'The average precision score of RF is: {aps_rf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the things we've learned:\n",
    "- How to train a binary classifier\n",
    "- Choosing an appropriate metric for the task\n",
    "- Evalutating classifiers using cross-validation\n",
    "- Selecting appropriate precision/recall tradeoffs\n",
    "- Comparing models using ROC curves and ROCAUC scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi class model: detecting more than 5s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen some of the major tools and concepts in classification, let's try for something harder: predicting all the classes in MNIST. The ideas are all the same, but the difficulty for our model is much higher. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance metrics we use for multi class predictions are derived from the binary classification metrics above, but averaged over all the classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to use scikit-learn models for multi-class predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn trained 10 binary classifiers for us, and used them all on the `some_digit` example. The class whose decision score was highest was outputted by `predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.decision_function([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(sgd_clf.decision_function([some_digit]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same with a random forest classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.fit(X_train, y_train)\n",
    "forest_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the list of probabilities that the random forest assigned to each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.predict_proba([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and notice that the fifth class (corresponding to 5s) got assigned the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, we can evaluate the classifiers as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(forest_clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the two classifiers confusion matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_train_pred = cross_val_predict(sgd_clf, X_train, y_train, cv=3)\n",
    "forest_train_pred = cross_val_predict(forest_clf, X_train, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_cm = confusion_matrix(y_train, sgd_train_pred)\n",
    "forest_cm = confusion_matrix(y_train, forest_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = [sgd_cm, forest_cm]\n",
    "titles = [\"SGDClassifier\", \"Random Forest\"]\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    plot_confusion_matrix(cms[i], classes=sgd_clf.classes_, ax=ax, title=titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both confusion matrices look pretty good: most of the images are on the diagonal. However, there are errors, and there seems to be some patterns.. Let's take a closer look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As a side note: these are not the kinds of models you would use for computer vision problems these days. You may have heard of **\"deep learning\"** and the **revolution** it has caused in machine learning during the last couple of years. Where it started, and where it's made the most impact until now, is in computer vision. In the last part of the course we'll have a look at deep learning, and study models that can easily reach above 99.7% accuracy on MNIST. Which means that the model makes a mistake on *less than 30 images of the 10.000 test images*! And for some of these images, if we take a look ourself, we would agree with the machine's predictions. Modern deep learning has blown away MNIST as a benchmark by essentially \"solving\" it. But that's for later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can zoom in on the errors in the confusion matrices by disregarding the dominating main diagonal. \n",
    "\n",
    "First we have to scale each value by the number of images in the corresponding class, to properly compare error rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = sgd_cm.sum(axis=1, keepdims=True)\n",
    "norm_sgd_cm = sgd_cm / row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_confusion_matrix(norm_sgd_cm, classes=sgd_clf.classes_, title=\"Error matrix\", labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice several intersting things:\n",
    "- Bright columns for 8 and 9 --> many images get misclassified as 8 or 9s.\n",
    "- Bright rows for 8 and 9 --> many 8s and 9s are often confused with other digits\n",
    "- The model confuses for example 7s with 9s and 3s with 5s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which gives us some ideas for improving our model. E.g. improve the 8 and 9 classifications, and fix the 3/5 confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing individual errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some 3s and 5s. Some 3s that the model got correct, some it confused for 5s, some 5s the model confused for 3s and some correct 5s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_errors(clf, cl_a, cl_b):\n",
    "    X_aa = X_train[(y_train == cl_a) & (sgd_train_pred == cl_a)] # Correct class a's\n",
    "    X_ab = X_train[(y_train == cl_a) & (sgd_train_pred == cl_b)] # a's predicted as b's\n",
    "    X_ba = X_train[(y_train == cl_b) & (sgd_train_pred == cl_a)] # b's predicted as a's\n",
    "    X_bb = X_train[(y_train == cl_b) & (sgd_train_pred == cl_b)] # Correct b's\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\n",
    "    plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\n",
    "    plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\n",
    "    plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_errors(sgd_clf, 3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SGDClassifier is a linear model. All it does is assign a weight per class to each pixel, and when it sees a new image it just sums up the wighted pixel intensities to get a score for each class. Since 3s and 5s only differ only by a few pixels, this model will easily confuse them. Same for 7s and 9s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_errors(sgd_clf, 7, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One would expect it to perform better on for example 1s and 4s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_errors(sgd_clf, 1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you've learned about some of the main practical machine learning ideas and techniques, and you're well-equipped to go out and attack real-world problems. \n",
    "\n",
    "However, we've treated the models as black boxes. We haven't really discussed **how they work**! It's important to know some of the how's to be an effective machine learning practitioner. \n",
    "\n",
    "In Part 3 we'll start to dig deeper. How are the models constructed? How are they trained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A *very ambitious* project if you feel up for it: repeat what we did above, but replace the images with skin cancer images from the ISIC archive (https://isic-archive.com/). You can find a convenient copy of the data here: https://challenge2018.isic-archive.com/. I recommend simplifying to a \"benign\" versus \"abnormal\" prediction task by reducing the number of categories in the data set to two. \n",
    "\n",
    "> The most difficult part of this problem is to get the data in a form in which you can feed it to machine learning models. Once that's done, to get a model that's not completely horrible you'll have to do some data preprocessing (like scaling the images and the pixel values). And also use a powerful model with well-tuned hyperparameters. Still you won't score very well. Unless you use deep learning.... See https://www.nature.com/articles/nature21056. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT158",
   "language": "python",
   "name": "dat158"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
