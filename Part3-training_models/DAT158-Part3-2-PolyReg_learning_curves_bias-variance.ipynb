{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alexander S. Lundervold, September 21st, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds on `DAT158-Part3-1-GradientDescent.ipynb` and covers the following topics:\n",
    "\n",
    "- **Polynomial regression:** Our data is often not neatly arranged along a straight line. We'll see how linear regression can be modified to handle data generated in more complex ways. We'll use this as motivation for the next three concepts below (which form the most important part of this discussion).\n",
    "\n",
    "\n",
    "- **Learning curves:** This is a very useful way to investigate whether your model is overfitting the training data.\n",
    "\n",
    "\n",
    "- **Bias/variance tradeoff:** An important concept in supervised machine learning, which will help you build better models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also Geron's notebook here: https://github.com/ageron/handson-ml/blob/master/04_training_linear_models.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook will repeat some of what we covered in Part 1, in the notebook `DAT158-Part1-Intro_to_ML.ipynb`.** You should study both of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We set up our standard environment below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To automatically reload modules defined in external files.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# To display plots directly in the notebook:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To make the notebook reproducible\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's make some data by using the quadratic function $2 + x + \\frac12$ with some added noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m,1) - 3\n",
    "y = 2 + X + 0.5 * X**2 + np.random.randn(m,1)\n",
    "\n",
    "plt.plot(X,y, 'b.')\n",
    "plt.axis([-3,3,0,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can try to fit a straight line to these points using `scikit-learn` as in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "theta = [lin_reg.intercept_, lin_reg.coef_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "...use it to predict on a new value and plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_new = np.array([[-3], [3]]) # We choose two new x-values to create our predicted straight line\n",
    "y_predict = lin_reg.predict(X_new)\n",
    "\n",
    "plt.plot(X_new, y_predict, 'r')\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "..not very impressive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is not surprising, as we're trying to fit a straight line to data generated from something more complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Luckily, we can still use `LinearRegression`, we just have to add more features that turns our straight line into a polynomial. That is, we do linear regression in a higher dimension.\n",
    "\n",
    "Instead of using the polynomial\n",
    "\n",
    "$$y = \\theta_0x_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n$$\n",
    "\n",
    "we use\n",
    "\n",
    "$$y = \\theta_0x_0 + \\theta_1 x_1 + \\theta_2 x_1^2 + \\theta_3 x_1^3  + \\cdots + \\theta_k x_1^k + \\theta_{k+1} x_2 + \\theta_{k+2} x_2^2 + \\cdots \\theta_{nk} x_n^k$$\n",
    "\n",
    "This is called **polynomial regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Polynomial regression by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lets try it out by adding a new feature to `X` that is the square of the values in `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_square = np.dstack((X, X**2))\n",
    "X_square = X_square[:,0,:] # Needed to make LinearRegression happy below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here are the first few values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_square[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Notice that the right column consists of the squares of the values in the left column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can use linear regression on these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lin_reg_square = LinearRegression()\n",
    "lin_reg_square.fit(X_square, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "...and then predict values for a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "# Add squares:\n",
    "X_new_square = np.dstack((X_new, X_new**2))\n",
    "X_new_square = X_new_square[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_new = lin_reg_square.predict(X_new_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Great!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that we have more parameters than for plain linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lin_reg_square.intercept_, lin_reg_square.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "variables": {
     "\"%.2f\" % lin_reg_square.coef_[0][0]": "0.93",
     "\"%.2f\" % lin_reg_square.coef_[0][1]": "0.56",
     "\"%.2f\" % lin_reg_square.intercept_[0]": "1.78"
    }
   },
   "source": [
    "Our model produced the function \n",
    "\n",
    "{{\"%.2f\" % lin_reg_square.intercept_[0]}} + {{\"%.2f\" % lin_reg_square.coef_[0][0]}} x + {{\"%.2f\" % lin_reg_square.coef_[0][1]}} x^2,\n",
    "\n",
    "while the true function was \n",
    "\n",
    "$2 + x + \\frac12 x^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This was the hard way to do it.. Of course, scikit-learn has built-in tools for creating polynomial features which lead to the exact same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_poly[:5] # Original feature of X and the square of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_new = np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Higher order polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There's no reason to restrict yourself to only degree 2 polynomials. It's straightforward to add polynomial features of any degree!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's try degree 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "degree=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "But first, it's important to scale the data when using high-dimensional models (and when using gradient descent, as discussed earlier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What we're doing is \n",
    "\n",
    "<img src=\"assets/pipeline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Once you're in a situation where you're taking more than a couple of steps to get from your data to predictions, you should use scikit-learn's `Pipeline` module, as you did in Assignment 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#The following corresponds to the above diagram:\n",
    "polynomial_regression = make_pipeline(PolynomialFeatures(degree=degree, include_bias=False),\n",
    "                                     StandardScaler(),\n",
    "                                     LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fit using the entire pipeline:\n",
    "polynomial_regression.fit(X, y)\n",
    "\n",
    "# Predict on new data (using the pipeline):\n",
    "y_new = polynomial_regression.predict(X_new)\n",
    "\n",
    "# Plot the result\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's make functions that investigates the effect of adding degrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_pipeline(degree):\n",
    "    poly_reg = make_pipeline(PolynomialFeatures(degree=degree, \n",
    "                                                include_bias=False),\n",
    "                                                StandardScaler(),\n",
    "                                                LinearRegression())\n",
    "    \n",
    "    return poly_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_poly_regression(degree, ax=None):\n",
    "    \n",
    "    polynomial_regression = get_pipeline(degree)\n",
    "    polynomial_regression.fit(X, y)\n",
    "    y_new = polynomial_regression.predict(X_new)\n",
    "    if not ax: fig, ax = plt.subplots()\n",
    "    ax.axis([-3, 3, 0, 10])\n",
    "    ax.plot(X, y, \"b.\")\n",
    "    ax.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "    ax.set_title(f'Degree {degree}')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_ = plot_poly_regression(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_ = plot_poly_regression(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_ = plot_poly_regression(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_ = plot_poly_regression(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_ = plot_poly_regression(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "degrees = [1, 2, 20, 300]\n",
    "fig, axes = plt.subplots(2,2, figsize=(14,10))\n",
    "for i,ax in enumerate(axes.flat):     \n",
    "    plot_poly_regression(degrees[i], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Notice that as we're increasing the degree we get a better and better fit to our data. However, as we get to higher and higher degrees, our model is less and less reasonable. It fits the training data well, but it tries to hard and ends up very different from what we know is the true data generating process (a polynomial of degree 2).\n",
    "\n",
    "This is, as we know, what's called **overfitting**. Our model fails to **generalize** as it gets more and more complex. \n",
    "\n",
    "> Even if a model fits well with the training data, it's not guaranteed to perform well on unseen test data. Even if it gets 100% accuracy on the training data! This is the problem of **generalization**: you're interested in models whos performance generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Earlier, we've used train-test-splits and cross-validation to help us detect when models were overfitting the training data. Now, let's look at another very useful tool for investigating model performance and doing model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The basic idea is to simultaneously measure the model's performance on the training data and on a validation data set as we increase the total data set size. It's the same concept as in human learning, where learning typically increases with experience:\n",
    "\n",
    "<img width=60% src=\"assets/wikipedia_learning_curves.png\"><br><small><center>Image from <a href=\"https://commons.wikimedia.org/wiki/File:Alanf777_Lcd_fig01.png\">Wikipedia</a></center></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To achieve this we\n",
    "1. Split the data into training and validation using `train_test_split`\n",
    "2. Calculate the mean squared error for the model on the training and validation data as we increase the data size from 1 to all data points.\n",
    "3. Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function from the textbook\n",
    "def plot_learning_curves(model, X, y):\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "    \n",
    "    # For storing the results\n",
    "    train_errors, val_errors = [], []\n",
    "    \n",
    "    # Calculate RMSE as we increase the data size\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "\n",
    "    # Plot the results\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "    plt.legend(loc=\"upper right\", fontsize=14) \n",
    "    plt.xlabel(\"Training set size\", fontsize=14)\n",
    "    plt.ylabel(\"RMSE\", fontsize=14)\n",
    "    plt.title('Learning curves')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's try it out on our data using polynomial regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curves_linreg(degree, axis=[0, 80, 0, 3]):\n",
    "    poly_reg = get_pipeline(degree)\n",
    "    _ = plot_poly_regression(degree)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plot_learning_curves(poly_reg, X, y)\n",
    "    plt.axis(axis)\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curves_linreg(degree=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curves_linreg(degree=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Two important points to notice based on these plots:**\n",
    "\n",
    "1. In the learning curve for standard linear regression (degree=1) we notice that the curves quickly stabilize to a relatively high error rate. Feeding the models more data doesn't help. This is how the learning curves of an **underfitting** model typically looks like. As we shall see, another way to say the same thing is that the model has **high bias**.\n",
    "\n",
    "\n",
    "2. When we use a high degree polynomial, for example degree=10, we got a learning curve where there's a gap between the training and validation performance. On the training set the model scores very well, but it fails to obtain a similar score on the validation set. This is typical for models that are **overfitting** the training data and failing to generalize to new data. In other words, the model has **high variance**, as explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: scikit-learn has a built-in method for creating learning curves. Have a look at <a href=\"http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\">Plotting Learning Curves</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# The bias/variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is a crucial concept in supervised machine learning. Good knowledge of this concept helps you become a better machine learning engineer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Bias and variance are two different sources of errors for machine learning models:\n",
    "\n",
    "\n",
    "* The **bias** of a model comes from the simplifying assumptions made by the model to make the target function easier to approximate. In a high bias model, the assumptions doesn't fit well with the data (for example, assuming that the data is linear when it's really quadratic), typically resulting in underfitting.\n",
    "\n",
    "\n",
    "* The **variance** of a model is an estimate of how much the model changes based on changes in the training data. In other words, how sensitive the model is to the precise data set. The high-degree polynomial above was very sensitive to the training data set, because of its many degrees of freedom. A high variance model will tend to overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"assets/biasvariance.png\"><br><small><center>Image from http://scott.fortmann-roe.com/docs/BiasVariance.html</center></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There's a third source: **irreducible error**, which comes from inherent uncertainty or noise in the data, which we can't fix unless we change the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## The tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Oversimplified models will tend to have high bias, underfitting both the training data and the test data. Increasing their complexity will lower their bias, but increase their variance, making them poor at generalization. \n",
    "\n",
    "What you want is to find the sweet spot where the total eror caused by the bias and the variance (and the irreducible error) is lowest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"assets/bias_variance_tradeoff.png\"><br><small><center>Image from http://scott.fortmann-roe.com/docs/BiasVariance.html</center></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Mathematically:** Assume there is a relationship of the form $y = f(X) + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,\\sigma_{\\epsilon})$ represents the error due to noise. \n",
    "\n",
    "Using linear regression, or any other model, we estimate a model $\\hat{f}(X)$ of $f(X)$. The expected squared predicted error at point x is $$\\mbox{Err}(x) = E\\big[\\big(y - \\hat{f}(x)\\big)^2\\big].$$\n",
    "\n",
    "This can be decomposed into bias and variance components: $$\\mbox{Err}(x) = \\big(E[\\hat{f}(x)] - f(x)\\big)^2 + E\\big[ \\big(\\hat{f}(x) - E[\\hat{f}(x)]\\big)^2 \\big]+ \\sigma_{\\epsilon}^2$$\n",
    "\n",
    "In other words:\n",
    "$$\\mbox{Err}(x) = Bias^2 + Variance + \\mbox{Irreducible error}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In our polynomial regression experiments above, the degree 2 polynomial had the smallest bias and variance, and was therefore the best model. We can verify this by plotting the mean squared error for increasing degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "poly_reg = get_pipeline(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "degrees = [1, 2, 3, 4, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_scores, validation_scores = validation_curve(poly_reg, X, y,\n",
    "                                                   param_name='polynomialfeatures__degree',\n",
    "                                                   param_range=degrees,\n",
    "                                                   scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(degrees, validation_scores.mean(axis=1), lw=2,\n",
    "         label='validation')\n",
    "plt.plot(degrees, train_scores.mean(axis=1), lw=2, label='training')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> How do we lower the variance of complex models? How do we force models to not overfit without losing generalization performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Regularization** is the answer, and we'll cover it in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Your turn!**\n",
    "- Find a good value for degree by using `GridSearchCV` on our `polynomial_regression` estimator.\n",
    "\n",
    "> **Hint:** the degree inside the `polynomialfeatures` part of the `polynomial_regression` can be accessed via `polynomialfeatures__degree`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT158",
   "language": "python",
   "name": "dat158"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
